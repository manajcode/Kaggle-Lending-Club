---
title: "Lending Club Loan History Challenge"
author: "Michael Najarro"
date: "6/11/2020"
output: pdf_document
---

```{r, echo = FALSE, message =FALSE}
library(pacman)
p_load(tidyverse,
       tinytex,
       Amelia,
       knitr,
       DataExplorer, 
       corrplot,
       magrittr,
       tictoc,
       beepr)
```

# *Objective*
The goal of this project is to predict whether a borrower will be at risk to default on his or her loan. Here I classify lending club members based on the response variable loan status.

# *Introduction*
Due to the limitations in computing power of my local machine, I was able to obtain a 10% portion of the of the 2012-2014 Lending club data; I applied a stratified random sample to the 3 years of the issue_d column and extracted those rows from the main data set. The procedures for collecting this data can be found in a seperate Rmarkdown file titled "LC_data_cleaning.Rmd." Note that I converted the response variable loan_status to a binary categorical variable called "borrower status" already.

## **Phase 1: collecting the data**

### 1.a) Import the lending club data to the R environment.

The chunk below uploads the 10% portion of the 2012-2014 Lending club data from an rds file:

```{r message=FALSE}
LCTF10 <- readRDS(file = "./cleaneddata.rds" )
```


### 1.b) Make a table of the variable names.

Note that two additional columns exist in the data; Issue_Month and Issue_Year. These two columns were created in the data processing stage in order to subset the data by the desired years.

```{r}
vari <- as.data.frame(colnames(LCTF10))
colnames(vari) <-  c("variables")

kable(vari, format = "html", caption = "fig 1. variables names of lending club data", align = 'l')
```


### 1.c) Make a table of the number of rows and columns in the dataframe.

```{r}
dimen <- data.frame(cbind(rows = nrow(LCTF10), columns = ncol(LCTF10)))

kable(dimen, format = "html", caption = "fig 2. dimensions of the lending club data", align = 'l')
```


## **Phase 2: Data exploration**

### 2.a) Investigate the descriptive statistics using an automated data exploration pacakage.

Here I use the package Data explorer to automatically create a report on a descriptive statistics analysis of the raw data. It is possible to do this analysis using the package treliscope as well.

```{r}
create_report(LCTF10, y = "loan_status")
```


### 2.b) Summarize the important features of the data.

There are several important features of the data:
 
    1. Time is measured by several variables; issue_date and last_credit_pull_date. For this analysis I will consider issue_date as the measure of time to subset the data for the years of 2012 to 2014.

    2. The response variable is borrower status.
  
    3. There 152 predictor variables. Of these, over half of the variables are continuous data types.
    
    4. There are 2.012 million NAs out of 6.44 million cells, or approximately 31.2% of the data is NAs. The NA columns can be seen in the data explorer report, however there are too many variables plotted to read the names of the columns that have 95% missing data.


## **Phase 3: data cleaning on the response variable and unique identifier columns**

### 3.a) Identify and remove all ID variables.

ID variables cannot be kept in for any analysis, as they will produce overfitting.

```{r}
LCTF10 <- LCTF10 %>% select(-id, -member_id)
```


### 3.b) Dealing with the response variable; any NAs?

Any records whose response variable is NA must be removed; no prediction can be made on an NA. Thus the first thing to do is to identify the unique values of the response and identify which rows have an NA value.

```{r}
table(LCTF10$borrower_status)

table(is.na(LCTF10$borrower_status))
```

There are no records containing an NA within the response column, so our response variable is clean.


## **Phase 4: Predictor variable reduction** 

Here I apply a process to remove columns that provide no information in the ability to classify loan borrowers as risky or safe. The steps are as follows:

    a. Evaluate the variables that have almost no ability to classify based on the percentage of NA values 
  
    b. Identify "repeat" variables, variablese that contain the same data of another column but in a different type (e.g. 1 vs "one")
    
    c. Identify variables that have too may classes or distinct values for a variable of type categorical or character; that is to say, too much variability for character or categorical varibles.
  
    d. Variables that have data that cannot be analyzed (e.g. columns with notes, aka "empty values")

    e. remove "loan status", the original response variable that was transformed into a binary classifier.
    
    f. remove any date columns within the data set; not helpful, similar to c.
    
    g. Identify the variables that contain a high degree of correlation with 1 or more other variables.
    
    h. Identify the variables that contain numeric or integer data but have too little variability.


### 4.a.1) Evaluate the percent of NAs in the predictor variables.

To begin, I identify the percent frequency of NAs per each predictor variable, and specifically present the variables whose percentage of NAs is greater than 50%. In general a variable whose missing more than half of its data cannot be used to predict an outcome, as the selection is a guess at best.


```{r}
percentnas <- sapply(LCTF10,function(x) round(sum(is.na(x))/nrow(LCTF10), digits = 2) )

percentnas <- as.data.frame(percentnas) %>%
  rename(percent = percentnas) %>%
  mutate(variable = rownames(.) ) %>%
  filter(percent > .49) %>%
  select(variable, percent) %>%
  arrange(desc(percent)) %>%
  mutate(count= 1:nrow(.)) %>%
  select(count, variable, percent)

kable(percentnas, caption = "Predictors with  high proportions of NAs", align = 'l')
```

There were 43 predictor variables whose percentage of NAs was greater than 50%.

Given this information, we can now filter out the variables whose percentage of NAs is greater than 50%.


### 4.a.2) Create a function to identify the predictor variables (column numbers) that have 50% or more NAs and remove them from the data set. 

```{r}
# create a function to determine which variables have
# less than or equal to 50% legit data:
d <- rep(0, ncol(LCTF10))
bad <- as.integer(rep(0, ncol(LCTF10)))
result<- as.character(rep(0, nrow(LCTF10)))

assess_bad_data<- function(df) {
d <<- (colSums(!is.na(LCTF10))/nrow(LCTF10))
bad <<- (which(d <=.50))
result <<- (colnames(LCTF10[bad]))
#return(result)
}
assess_bad_data(LCTF10)
```


 Below I check the function outputs and then remove the predictor variables from the data set based on column number.

```{r}
#how many bad columns are there?
length(result)

#which are the bad columns(by column number)?
bad

#what are the proprtions of legitimate data per bad column?
#d[result]

# toss the bad variables from td.
LCTF10 <- LCTF10[-(bad)]
```


Using the missmap function from Amelia gives a global view of the missing values across all variables in a colored grid pattern.

```{r}
# a visual approach to the number of NAs.
missmap(LCTF10)
```


### 4.b) Identify "repeat" variables

Repeat variables are variables that contain the same data, stored in different data types. Generally, it is advisable to keep quantified versions of duplicated variables and remove the character or string version, as the model will have to deal with many different combinations of levels, bogging the model and machine donwn.

To identify duplicates requires observing the kaggle Lending Club cookbook for variable definitions to find variables with similarl meanings.

A review of the varaibles within the data indicate several extremely similarly named predictors whose values are exact or similar to their counterparts:

    1.loan_amnt-The listed amount of the loan applied for by the borrower. If at some point in time, the credit department reduces the loan amount, then it will be reflected in this value.
    2.funded_amnt-The total amount committed to that loan at that point in time
    3.funded_amnt_inv-The total amount committed by investors for that loan at that point in time.
    4.total_pymnt-Payments received to date for total amount funded
    5.total_pymnt_inv-Payments received to date for portion of total amount funded by investors


variables ending containing "inv_" and "int_" signify  investor and interest; these variables can be kept but 
    
I'll keep the following variables:
funded_amnt
total_pymnt

```{r}
LCTF10 <- LCTF10 %>%
  select(-c(funded_amnt, funded_amnt_inv,total_pymnt_inv))
```


### 4.c) Identify and remove factor variables with too many levels or cahracter variables with too many distinct values.

Having too many levels in a factor can lead to overfitting, and become taxing for the model and your local machine's RAM and processor. Thus I will exclude predictor variables that contain over 6 levels. 

I first identify the columns that are of type factor and count the number of levels of each predictor variable.

```{r}
LCTF10 %>%
  select_if(is.factor) %>%
  #sapply(function(x) summary(x)/length(x))
  sapply(function(x) length(levels(x)))
```

I then keep only the variables with 5 or less levels. Notice that in this step I remove the original, unmodified predictor Loan_status, saving us a step later on.

```{r}
v<- LCTF10 %>%
  select_if(is.factor) %>%
  sapply(function(x) length(levels(x)))

q<- keep(v, function(x) x>5)
s<- names(q)
s
LCTF10 <- LCTF10 %>%
  select(-all_of(s))
```


### 4.d) Variables that have data that cannot be analyzed.

These predictors contain observational notes collected from bank employees. These columns were removed in step 4.c. 


### 4.e) Remove loan_status.

Note that this variable was already removed in step 4.c

```{r echo=FALSE, eval=FALSE}
#which(colnames(LCTF10)=="loan_status")
#LCTF10<-LCTF10[,-16]
```


### 4.f) Remove any date columns within the data set.

date columns, irrepsective of their type, produce excess complexity that produces no beneift to the analysis. Note that most of the date predictors were removed in step 4.c.

thus I only remove predictor variable Issue_month.

```{r}
LCTF10 <- LCTF10 %>%
  select(-c(Issue_Month))
```

    
### 4.g) Identify the variables that contain a high degree of correlation with 1 or more other variables.

In order to perform the correlation matrix, I need to first address the issue of missing data, or NAs within the data frame. a missing map indicates a relatively small percent of indivuals have missing data for only a few predictors.

```{r}
missmap(LCTF10)
```

A reasonable approach would be to impute values. Here I take a conservative approach and remove records that contain an NA.

```{r}
LCTF10 <- na.omit(LCTF10)
```


Below I create a correlation matrix of all numeric variables and Identify which predictor variables that have the greatest number of unique intertactions, when the pearson's correlation value is greater than 0.5. AS we can see columns 3,11,15,18,19,20,31,47,48,49,50,52,53,64,55, and 64 have the highest number of correlations with other variables.

```{r}
# correlation matrix
a<- LCTF10 %>%
  select_if(is.numeric)

corrplot(cor(a), method = "color")

b<- as.matrix(cor(a))
which(b > 0.5, arr.ind = TRUE)
```

A reveiew of the actual correlation measures indicate very high correlation for most.

```{r}
which(b > 0.5,arr.ind = TRUE)
hc<-which(b > 0.5) 
b[hc]
```


I further analyzed the correlation in the inverse relationship but found very few predictor variables containing extreme correlations. I left these variables alone. 

```{r echo=FALSE}
which(b < -0.5, arr.ind = TRUE)
lc<-which(b < -0.5) 
b[lc]
```


Now I remove the aforementioned "columns" or predictor variables from LCTF10 that have high correlations. Note that I excluded variable loan_amnt as that column represents the amount of money that was lent and is important for analysis.

```{r}
highcor<-colnames(b[,c(3,11,15,18,19,20,31,47,48,49,50,52,53,64,55,64)])

LCTF10 <- LCTF10 %>%
  select(-all_of(highcor))
```



### 4.h) Identify the variables that contain numeric or integer data but have too little variability.

Because I remvoed the NAs previosuly, I look for variables who have zero variance. 

```{r}
#clear your environ
rm(bad,d,q,result,s,v,assess_bad_data,a,b,hc,highcor,lc)

# variance of numeric variables
rr<-LCTF10 %>%
  select_if(is.numeric) %>%
  sapply(function(x) var(x))

LCTF10 <- LCTF10 %>%
  select(-names(which(rr == 0)))
```


Selecting out variables close to zero may conflate low variability with a reasonable distribution of data, given the particular measurement. Thus for the reamaining numeic variables I evaluate the distributions and toss variables that have highly skewed distributions.

```{r}
#LCTF10 %>%
#  keep(is.numeric) %>%
#  gather() %>%
#  ggplot(aes(value)) +
#    facet_wrap(~ key, scales = "free") +
#    geom_density()

#pull out the names of the numeric columns
w<-LCTF10 %>%
select_if(is.numeric) %>%
  names()

#create a df of numeric colums
k<- LCTF10 %>%
select_if(is.numeric)

#identify the histograms with extreme dsitributions
#10,11-13-16,21-24,27,29,30
sapply(k[1:30], FUN=hist)

#41-43,48-50
sapply(k[31:51], FUN=hist)

#pull out the histogram names of low diversity
ww<- w[c(10,11,13,14,15,16,21,22,23,24,27,29,30,41,42,43,48,49,50)]

LCTF10 <-LCTF10 %>%
  #select(-c(ww))
  select(-all_of(ww))

rm(k,rr,w,ww,vari,percentnas,dimen)
```



##**Phase 5: Generate Model on training Data**

The goal of this machine learning model is to classify a borrower on their ability to pay back their loan in a supervised machine learning model, given all of the predictor variables and the response variable of two classes: "risky" and "safe" clients.

### 5.a) clean up the predictor variable

Before beginning any machine learning algorithm, the response variable needs to be converted from character to a factor type variable.

```{r}
# convert the response to a factor
LCTF10$borrower_status <- as.factor(LCTF10$borrower_status)

# create a new vector that avoids the space in the response variable's levels
#library(car)
p_load(car, randomForest, caret, e1071)
possible_default <- as.factor(recode(as.vector(LCTF10$borrower_status), "'risky client'='risk'; 'safe client'='safe'"))

#check to make sure the newest predictor matches previous 
str(possible_default)
table(possible_default)
table(LCTF10$borrower_status)

# add the vector to LCTF10
LCTF10 <- LCTF10 %>%
  mutate(chance_default = possible_default)

table(LCTF10$borrower_status)
table(LCTF10$chance_default)

# now drop the borrower status
LCTF10 <- LCTF10 %>%
  select(-borrower_status)

saveRDS(object = LCTF10, file = "data_for_rf.rds")
```


### 5.b) I can now create a test and training data set for model development.

I create the test and training data using a 25:75 percent split on the data.

```{r}
# create the test data
n <- nrow(LCTF10)

# create the test data
test <- sample.int(n, size = round(0.25 * n))
test_set <- LCTF10[test, ]
nrow(test_set)

#create the training data
train_set <- LCTF10[-test,]
nrow(train_set)
```


### 5.c) Here I will create a random forest model on the training data using the random forest package.

```{r}
rm(rf,possible_default)

set.seed(2221)
tic()
rf = randomForest(chance_default~., data = train_set)
toc()

print(rf) 

attributes(rf)
rf$err.rate

plot(rf) # by about 325 trees should be stable; tune mtry.
```


##**Phase 6: Run and Evaluate Model on Test Data**

### 6.a) Run and evaluate how the model does on the test data.

```{r}
tic()
p2 <- predict(rf, test_set)
toc()

confusionMatrix(p2, test_set$chance_default)
```


##*Phase 7: Improve Model Performance**

### 7.a) Now tune the random forest

```{r}
set.seed(9006)
tic()
m <- caret::train(y = train_set$chance_default,
                  x = train_set[,colnames(train_set) != 'chance_default'],
                  trControl = trainControl(method="oob"),
                  method = "rf")
toc()
m
```


### 7.b) Revise the random forest model with new info.

Here I create a new random forest model with a new mtry set at 23. 

```{r}
#remove old model and LCTF10
#rm(rf, LCTF10)

tic()
rf2 = randomForest(chance_default~., data = train_set, ntree = 325, mtry = 23)
toc()

rf2 
```


### 7.c) Evaluate new model's performance on the test data.

```{r}
p2 <- predict(rf2, test_set)

confusionMatrix(p2, test_set$chance_default)
```


### 7.d) Get info on average trees of rf2 model.

```{r}
# typical number of levels in a tree.
hist(treesize(rf2), main = "number of nodes for the trees", col = "green")

# Plot importance of each variable to rf2 on model accuracy and puirty (gini index)
varImpPlot(rf2)
```

*Test of the model on 2015 data*

Given the cleaned up model on the 2012-2014 data, I now will apply the model to a data set it never saw before, the 2015 data.

Unforutnately, the 2015 data will require some minor cleaning to prepare for testing (i.e. collapse and modify the resposne variable, remove high cardinality variables, remove nas...etc.)

At the present moment I do not have a machine capable of reloading the complete lending club data so I am putting this portion of the analysis on hold.


*Alternative random forest analysis using Ranger*

This section is an alternative approach to implementing the Random Forest classfication algorithm using a more modern implementation of the Random Forest called Ranger.

## **Phase:1 clean the environment and load in your refined data**

```{r}
#clean environment
rm(list=ls())
gc()

# load back your data
refineddf <- readRDS(file = "./data_for_rf.rds" )
```


## **Phase 2: create test adn training data**

### 2.a) split your data into training and test sets

```{r}
# create the test data
n <- nrow(refineddf)

# create the test data
test <- sample.int(n, size = round(0.25 * n))
test_set <- refineddf[test, ]
nrow(test_set)

#create the training data
train_set <- refineddf[-test,]
nrow(train_set)
```


## **Phase 3: Grow the Random Forest model on the Training data**

### 3.a) Build the model on training data 

```{r}
library(ranger)

tic()
rf1 <- ranger(chance_default ~ .,
              data = train_set,
              num.trees = 100,
              mtry = 5,
              write.forest = TRUE,
              importance="impurity",
              min.node.size = 1,
              max.depth = 0,
              splitrule= "gini",
              num.threads = 2,
              classification=TRUE,
              seed = 2020,
              regularization.factor = 1
              )
toc()
beep(sound=2)
```


## **Phase 4: test the predicted values from training model onto the test data**

### 4.a) Check our the model first

```{r}
print(rf1)

head(predictions(rf1))
```


### 4.b) Compare the model's predicted values relative to the test data

```{r}
pred_val <- predict(rf1, data = test_set)

#pivot table of classification results
table(test_set$chance_default, pred_val$predictions)

# percent classifications
prop.table(table(test_set$chance_default, pred_val$predictions))

#generate a binary classifier for whether the probability
# of predictions is greater than 50%
#stored_prepared <- c(rep(0, length(test_set$chance_default)))
#stored_prepared[pred_val > 0.5] <- "1"


# confusion matrix results
confusion_matrix <- function(stored_pred, ys){
  tb <- table(stored_pred, ys)
  print(addmargins(tb))
  accuracy <- (tb[1,1] + tb[2,2])/sum(tb)
  sensitivity <- tb[2,2]/sum(tb[,2])
  specificity <- tb[1,1]/sum(tb[,1])
  
  print(paste("accuracy=",round(accuracy, digits =4)))
  print(paste("sensitivity=",round(sensitivity, digits = 4)))
  print(paste("specificity=",round(specificity, digits = 4)))
}

confusion_matrix(stored_pred = pred_val$predictions,
                 ys = test_set$chance_default)
```


